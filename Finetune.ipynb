{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch pandas numpy datasets transformers peft mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps.\n",
      "\n",
      "GPT-2 QuoteBot Fine-Tuning Setup:\n",
      "\n",
      "Loading the dataset...\n",
      "Dataset loaded: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['quote', 'author', 'tags'],\n",
      "        num_rows: 2508\n",
      "    })\n",
      "})\n",
      "Train samples: 2508\n",
      "\n",
      "Sample data structure: dict_keys(['quote', 'author', 'tags'])\n",
      "Sample quote: {'quote': '“Be yourself; everyone else is already taken.”', 'author': 'Oscar Wilde', 'tags': ['be-yourself', 'gilbert-perreira', 'honesty', 'inspirational', 'misattributed-oscar-wilde', 'quote-investigator']}\n",
      "\n",
      "Found existing fine-tuned model at: ./quotebot-gpt2-lora.\n",
      "Loading fine-tuned QuoteBot...\n",
      "Fine-tuned model loaded successfully.\n",
      "Model loaded on device: mps:0\n",
      "\n",
      "Skipping training, using existing fine-tuned model...\n",
      "\n",
      "Evaluating model perplexity...\n",
      "Average Loss: 2.5214\n",
      "Perplexity: 12.4466\n",
      "QuoteBot loaded from existing model.\n",
      "\n",
      "QuoteBot is set up and ready. Enter topics to generate quotes.\n",
      "Type 'quit' to exit.\n",
      "\n",
      "QuoteBot says:\n",
      "Topic: love\n",
      "Quote: \"“Love can be the most beautiful thing you can find.”\"\n",
      "The system ends.\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 QuoteBot Fine-tuning with PEFT (LoRA)\n",
    "# Here, we are fine-tuning GPT-2 to generate quote-style outputs using the english_quotes dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, \n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling, pipeline)\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}.\")\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "os.environ[\"MLFLOW_TRACKING_DISABLED\"] = \"true\"\n",
    "MODEL_NAME = \"gpt2\"\n",
    "DATASET_NAME = \"Abirate/english_quotes\"\n",
    "OUTPUT_DIR = \"./quotebot-gpt2-lora\"\n",
    "MAX_LENGTH = 128\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "SAVE_STEPS = 500\n",
    "print(\"\\nGPT-2 QuoteBot Fine-Tuning Setup:\\n\")\n",
    "print(\"Loading the dataset...\")\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "print(f\"Dataset loaded: {dataset}\")\n",
    "print(f\"Train samples: {len(dataset['train'])}\")\n",
    "sample = dataset['train'][0]\n",
    "print(f\"\\nSample data structure: {sample.keys()}\")\n",
    "print(f\"Sample quote: {sample}\")\n",
    "def check_existing_model():\n",
    "    model_files = ['adapter_config.json', 'adapter_model.bin'] \n",
    "    if os.path.exists(OUTPUT_DIR):\n",
    "        existing_files = [f for f in model_files if os.path.exists(os.path.join(OUTPUT_DIR, f))]\n",
    "        return len(existing_files) > 0\n",
    "    return False\n",
    "if check_existing_model():\n",
    "    print(f\"\\nFound existing fine-tuned model at: {OUTPUT_DIR}.\")\n",
    "    print(\"Loading fine-tuned QuoteBot...\")    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\")\n",
    "    model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(f\"Fine-tuned model loaded successfully.\")\n",
    "    print(f\"Model loaded on device: {next(model.parameters()).device}\")    \n",
    "    SKIP_TRAINING = True\n",
    "else:\n",
    "    print(\"\\nNo existing fine-tuned model found. Loading base model for training...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\")\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token    \n",
    "    print(f\"Model loaded on device: {next(model.parameters()).device}\")\n",
    "    print(f\"Model parameters: {model.num_parameters():,}\")\n",
    "    SKIP_TRAINING = False\n",
    "if not SKIP_TRAINING:\n",
    "    def preprocess_function(examples):\n",
    "        texts = []\n",
    "        for quote, author, tags in zip(examples['quote'], examples['author'], examples['tags']):\n",
    "            if tags and len(tags) > 0:\n",
    "                topic = tags[0] if isinstance(tags, list) else str(tags)\n",
    "            else:\n",
    "                topic = \"wisdom\"\n",
    "            topic = str(topic).strip()[:50]\n",
    "            quote = str(quote).strip()\n",
    "            text = f\"Topic: {topic}\\nQuote: \\\"{quote}\\\"<|endoftext|>\"\n",
    "            texts.append(text)\n",
    "        all_input_ids = []\n",
    "        all_attention_masks = []        \n",
    "        for text in texts:\n",
    "            encoded = tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                padding=False,\n",
    "                return_tensors=None)\n",
    "            if len(encoded['input_ids']) >= 10:\n",
    "                all_input_ids.append(encoded['input_ids'])\n",
    "                all_attention_masks.append(encoded['attention_mask'])        \n",
    "        return {\n",
    "            'input_ids': all_input_ids,\n",
    "            'attention_mask': all_attention_masks,\n",
    "            'labels': [ids.copy() for ids in all_input_ids]}\n",
    "    print(\"\\nPreprocessing the dataset...\")\n",
    "    tokenized_dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        batch_size=100, \n",
    "        remove_columns=dataset['train'].column_names,\n",
    "        desc=\"Tokenizing\",\n",
    "        num_proc=1)\n",
    "    print(f\"Tokenized dataset: {tokenized_dataset}\")\n",
    "    print(f\"Dataset size: {len(tokenized_dataset['train'])}\")\n",
    "    sample_tokenized = tokenized_dataset['train'][0]\n",
    "    sample_text = tokenizer.decode(sample_tokenized['input_ids'])\n",
    "    print(f\"\\nSample tokenized text: {sample_text}\")\n",
    "    print(f\"Input IDs length: {len(sample_tokenized['input_ids'])}\")\n",
    "    print(f\"Labels length: {len(sample_tokenized['labels'])}\")\n",
    "    print(f\"Input IDs type: {type(sample_tokenized['input_ids'])}\")\n",
    "    print(f\"Labels type: {type(sample_tokenized['labels'])}\")\n",
    "    print(f\"First few input IDs: {sample_tokenized['input_ids'][:10]}\")\n",
    "    print(\"\\nSetting up PEFT (LoRA) configuration...\")\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=16, lora_alpha=32, lora_dropout=0.1,\n",
    "        target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"])\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    @dataclass\n",
    "    class DataCollatorForCausalLM:\n",
    "        tokenizer: Any\n",
    "        max_length: int = MAX_LENGTH\n",
    "        def __call__(self, examples: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "            input_ids = [example['input_ids'] for example in examples]\n",
    "            labels = [example['labels'] for example in examples]            \n",
    "            max_len = min(max(len(ids) for ids in input_ids), self.max_length)            \n",
    "            padded_input_ids = []\n",
    "            padded_labels = []\n",
    "            attention_masks = []            \n",
    "            for ids, lbls in zip(input_ids, labels):\n",
    "                if len(ids) > max_len:\n",
    "                    ids = ids[:max_len]\n",
    "                    lbls = lbls[:max_len]                \n",
    "                padding_length = max_len - len(ids)                \n",
    "                padded_ids = ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "                padded_lbls = lbls + [-100] * padding_length  \n",
    "                attention_mask = [1] * len(ids) + [0] * padding_length                \n",
    "                padded_input_ids.append(padded_ids)\n",
    "                padded_labels.append(padded_lbls)\n",
    "                attention_masks.append(attention_mask)\n",
    "            return {\n",
    "                'input_ids': torch.tensor(padded_input_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_masks, dtype=torch.long),\n",
    "                'labels': torch.tensor(padded_labels, dtype=torch.long)}\n",
    "    data_collator = DataCollatorForCausalLM(tokenizer=tokenizer, max_length=MAX_LENGTH)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=50,\n",
    "        save_steps=SAVE_STEPS,\n",
    "        save_total_limit=2,\n",
    "        prediction_loss_only=True,\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=False,  \n",
    "        fp16=False,  \n",
    "        bf16=True,\n",
    "        dataloader_num_workers=0,  \n",
    "        report_to=[],  \n",
    "        load_best_model_at_end=False,\n",
    "        warmup_steps=100,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        disable_tqdm=False, \n",
    "        log_level=\"warning\",)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,)\n",
    "    print(\"\\nStarting training...\")\n",
    "    try:\n",
    "        import mlflow\n",
    "        mlflow.end_run()\n",
    "    except:\n",
    "        pass\n",
    "    trainer.train()\n",
    "    print(\"\\nSaving the model...\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    print(f\"Model saved to: {OUTPUT_DIR}.\")\n",
    "else:\n",
    "    print(\"\\nSkipping training, using existing fine-tuned model...\")\n",
    "def generate_quote(topic: str, max_length: int = 100, temperature: float = 0.8):\n",
    "    prompt = f\"Topic: {topic}\\nQuote:\"\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            top_k=50,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            num_return_sequences=1)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "def interactive_quote_generator():\n",
    "    print(\"\\nQuoteBot is set up and ready. Enter topics to generate quotes.\")\n",
    "    print(\"Type 'quit' to exit.\")\n",
    "    while True:\n",
    "        topic = input(\"\\nEnter a topic (type 'quit' or 'exit' to end the system): \").strip()\n",
    "        if topic.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"The system ends.\")\n",
    "            break\n",
    "        if topic:\n",
    "            try:\n",
    "                quote = generate_quote(topic, max_length=100, temperature=0.8)\n",
    "                print(f\"\\nQuoteBot says:\")\n",
    "                print(f\"{quote}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating quote: {e}.\")\n",
    "        else:\n",
    "            print(\"Please enter a valid topic.\")\n",
    "def evaluate_model_perplexity():\n",
    "    print(\"\\nEvaluating model perplexity...\")\n",
    "    dataset = load_dataset(DATASET_NAME)\n",
    "    def preprocess_function(examples):\n",
    "        texts = []\n",
    "        for quote, author, tags in zip(examples['quote'], examples['author'], examples['tags']):\n",
    "            if tags and len(tags) > 0:\n",
    "                topic = tags[0] if isinstance(tags, list) else str(tags)\n",
    "            else:\n",
    "                topic = \"wisdom\"\n",
    "            topic = str(topic).strip()[:50]\n",
    "            quote = str(quote).strip()\n",
    "            text = f\"Topic: {topic}\\nQuote: \\\"{quote}\\\"<|endoftext|>\"\n",
    "            texts.append(text)\n",
    "        all_input_ids = []\n",
    "        all_attention_masks = []\n",
    "        for text in texts:\n",
    "            encoded = tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                padding=False,\n",
    "                return_tensors=None)\n",
    "            if len(encoded['input_ids']) >= 10:\n",
    "                all_input_ids.append(encoded['input_ids'])\n",
    "                all_attention_masks.append(encoded['attention_mask'])\n",
    "        return {\n",
    "            'input_ids': all_input_ids,\n",
    "            'attention_mask': all_attention_masks,\n",
    "            'labels': [ids.copy() for ids in all_input_ids]}\n",
    "    tokenized_dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        batch_size=100,\n",
    "        remove_columns=dataset['train'].column_names,\n",
    "        desc=\"Tokenizing for evaluation\",\n",
    "        num_proc=1)\n",
    "    @dataclass\n",
    "    class DataCollatorForCausalLM:\n",
    "        tokenizer: Any\n",
    "        max_length: int = MAX_LENGTH\n",
    "        def __call__(self, examples: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "            input_ids = [example['input_ids'] for example in examples]\n",
    "            labels = [example['labels'] for example in examples]\n",
    "            max_len = min(max(len(ids) for ids in input_ids), self.max_length)\n",
    "            padded_input_ids = []\n",
    "            padded_labels = []\n",
    "            attention_masks = []\n",
    "            for ids, lbls in zip(input_ids, labels):\n",
    "                if len(ids) > max_len:\n",
    "                    ids = ids[:max_len]\n",
    "                    lbls = lbls[:max_len]\n",
    "                padding_length = max_len - len(ids)\n",
    "                padded_ids = ids + [self.tokenizer.pad_token_id] * padding_length\n",
    "                padded_lbls = lbls + [-100] * padding_length\n",
    "                attention_mask = [1] * len(ids) + [0] * padding_length\n",
    "                padded_input_ids.append(padded_ids)\n",
    "                padded_labels.append(padded_lbls)\n",
    "                attention_masks.append(attention_mask)\n",
    "            return {\n",
    "                'input_ids': torch.tensor(padded_input_ids, dtype=torch.long),\n",
    "                'attention_mask': torch.tensor(attention_masks, dtype=torch.long),\n",
    "                'labels': torch.tensor(padded_labels, dtype=torch.long)}\n",
    "    data_collator = DataCollatorForCausalLM(tokenizer=tokenizer, max_length=MAX_LENGTH)\n",
    "    eval_dataset = tokenized_dataset['train'].select(range(100))\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset, \n",
    "        batch_size=2, \n",
    "        collate_fn=data_collator)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item() * batch['input_ids'].numel()\n",
    "            total_tokens += batch['input_ids'].numel()\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Perplexity: {perplexity:.4f}\")\n",
    "    return perplexity\n",
    "perplexity = evaluate_model_perplexity()\n",
    "if not SKIP_TRAINING:\n",
    "    def make_json_serializable(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            return {k: make_json_serializable(v) for k, v in obj.items()}\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            return [make_json_serializable(item) for item in obj]\n",
    "        elif isinstance(obj, set):\n",
    "            return list(obj) \n",
    "        elif hasattr(obj, '__dict__'):\n",
    "            return make_json_serializable(obj.__dict__)\n",
    "        elif isinstance(obj, (torch.dtype, type)):\n",
    "            return str(obj)\n",
    "        else:\n",
    "            try:\n",
    "                json.dumps(obj)  \n",
    "                return obj\n",
    "            except TypeError:\n",
    "                return str(obj)\n",
    "    model_info = {\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"dataset\": DATASET_NAME,\n",
    "        \"peft_config\": make_json_serializable(peft_config.to_dict()),\n",
    "        \"training_config\": {\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"num_epochs\": NUM_EPOCHS,\n",
    "            \"max_length\": MAX_LENGTH,\n",
    "            \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,},\n",
    "        \"perplexity\": float(perplexity),\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"num_epochs\": NUM_EPOCHS}\n",
    "    with open(f\"{OUTPUT_DIR}/model_info.json\", \"w\") as f:\n",
    "        json.dump(model_info, f, indent=2)\n",
    "    print(f\"\\nModel info saved to: {OUTPUT_DIR}/model_info.json.\")\n",
    "if SKIP_TRAINING:\n",
    "    print(\"QuoteBot loaded from existing model.\")\n",
    "else:\n",
    "    print(\"QuoteBot training completed.\")\n",
    "interactive_quote_generator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
